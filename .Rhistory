if (NROW(of_interest) == 0) stop('no topic to look for')
cat('\n\n 2. filtering out news \n')
pb <- txtProgressBar(min = 0, max = NROW(of_interest), style = 3, width = 50, char = "=")
for(i in 1:NROW(of_interest)){
i.titles <- all_title[grep(of_interest[i], all_title)]
if(length(i.titles) > 0) my_news[[of_interest[i]]] <- i.titles
setTxtProgressBar(pb, i)
}
all_title
TermDocumentMatrix
tdm <- TermDocumentMatrix(all_title[1])
all_title
class(all_title)
tdm <- TermDocumentMatrix(all_title[1])
tdm <- TermDocumentMatrix(all_title)
str(all_title)
as.vector(all_title)
prova <- str(all_title)
tdm <- TermDocumentMatrix(prova)
tdm <- TermDocumentMatrix('ciao come ti va')
prova <-Corpus(all_title)
?Corpus
corpus <- iconv(all_title)
corpus <- Corpus(VectorSource(corpus))
tdm <- TermDocumentMatrix(corpus)
inspect(corpus[1:5])
corpus <- iconv(all_title)
corpus <- Corpus(VectorSource(corpus))
tdm <- TermDocumentMatrix(corpus)
tdm <- as.matrix(TermDocumentMatrix(corpus))
tdm[1:10, 1:20]
apply(tdm, 1, sum)
word_count <- apply(tdm, 1, sum)
word_count
quantile(word_count)
quantile(word_count .9)
quantile(word_count, .9)
quantile(word_count, .95)
quantile(word_count, .99)
word_count
length(word_count)
hist(word_count)
hist(word_count[word_count > 1])
hist(word_count[word_count > 2])
hist(word_count[word_count > 3])
hist(word_count[word_count > 30])
sort(word_count)[1:10]
sort(word_count, decreasing = TRUE)[1:10]
stopwords('english')
corpus <- iconv(all_title)
corpus <- Corpus(VectorSource(corpus))
corpus <- tm_map(corpus, removeWords, stopwords('english'))
tdm <- as.matrix(TermDocumentMatrix(corpus))
word_count <- apply(tdm, 1, sum)
sort(word_count, decreasing = TRUE)[1:10]
corpus <- iconv(all_title)
corpus <- Corpus(VectorSource(corpus))
corpus <- tm_map(cleanset, stemDocument)
corpus <- iconv(all_title)
corpus <- Corpus(VectorSource(corpus))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
tdm <- as.matrix(TermDocumentMatrix(corpus))
word_count <- apply(tdm, 1, sum)
sort(word_count, decreasing = TRUE)[1:10]
to_rem <- c('stock','market','read','min','opinion','say','high','low',
'share','price','bank')
corpus <- iconv(all_title)
corpus <- Corpus(VectorSource(corpus))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, removeWords, to_rem)
tdm <- as.matrix(TermDocumentMatrix(corpus))
word_count <- apply(tdm, 1, sum)
sort(word_count, decreasing = TRUE)[1:10]
to_rem <- c('stock','market','read','min','opinion','say','high','low',
'share','price','bank','trade','record','new','report','invest',
'will','whi')
corpus <- iconv(all_title)
corpus <- Corpus(VectorSource(corpus))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, removeWords, to_rem)
tdm <- as.matrix(TermDocumentMatrix(corpus))
word_count <- apply(tdm, 1, sum)
sort(word_count, decreasing = TRUE)[1:10]
word_count[word_count > 1 & word_count < 5]
word_count[word_count > 3 & word_count < 5]
word_count[word_count > 20 & word_count < 50]
word_count[word_count > 10 & word_count < 50]
word_count[word_count > 10 & word_count < 20]
data.frame(word_count[word_count > 10 & word_count < 20])
today_trend <- data.frame(word_count[word_count > 10 & word_count < 20])
to_rem <- c('stock','market','read','min','opinion','say','high','low',
'share','price','bank','trade','record','new','report','invest',
'will','whi','buy','investor')
corpus <- iconv(all_title)
corpus <- Corpus(VectorSource(corpus))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, removeWords, to_rem)
tdm <- as.matrix(TermDocumentMatrix(corpus))
word_count <- apply(tdm, 1, sum)
today_trend
write.csv(today_trend, 'todays_trend.csv')
rm(list = ls())
setwd("/home/ndrecord/Dropbox/r_FINNEWS")
# source('functions_dev.R')
source('functions_source.R')
suppressMessages(library(stringr))
suppressMessages(library(rvest))
suppressMessages(library(tm))
warning('synonimus implementation')
all_title <- c()
sources <- read.csv('sources.csv')
# N <- nrow(sources); sources <- sources[N,]; sources
if(NROW(sources) == 0) stop('no source to scrape')
cat('\n 1. retreiving news from', NROW(sources), 'sources \n ')
pb <- txtProgressBar(min = 0, max = nrow(sources), style = 3, width = 50, char = "=")
for(i in 1:nrow(sources)){
i.url <- sources$url[i]
i.node_type <- sources$node_type[i]
my_titles_raw <- news_retreive(url = i.url, node_type = i.node_type)
if((length(my_titles_raw) > 0)){
all_title <- unique(c(all_title,  text_prepro(x = my_titles_raw))) # remove duplicates
} else {
warning(paste("\n source", i, "couldn't be queried"))
}
setTxtProgressBar(pb, i)
}
to_rem <- c('stock','market','read','min','opinion','say','high','low',
'share','price','bank','trade','record','new','report','invest',
'will','whi','buy','investor')
corpus <- iconv(all_title)
corpus <- Corpus(VectorSource(corpus))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, removeWords, to_rem)
tdm <- as.matrix(TermDocumentMatrix(corpus))
word_count <- apply(tdm, 1, sum)
today_trend <- data.frame(word_count[word_count > 10 & word_count < 20])
today_trend
suppressMessages(library(doParallel))
suppressMessages(library(foreach))
ncore <- 8
workers <- makeCluster(ncore)
registerDoParallel(workers)
all_title <- c()
sources <- read.csv('sources.csv')
# N <- nrow(sources); sources <- sources[N,]; sources
if(NROW(sources) == 0) stop('no source to scrape')
cat('\n 1. retreiving news from', NROW(sources), 'sources \n ')
pb <- txtProgressBar(min = 0, max = nrow(sources), style = 3, width = 50, char = "=")
all_title <- c()
sources <- read.csv('sources.csv')
# N <- nrow(sources); sources <- sources[N,]; sources
if(NROW(sources) == 0) stop('no source to scrape')
cat('\n 1. retreiving news from', NROW(sources), 'sources \n ')
x <- foreach(i = 1:nrow(sources)) %dopar% {
i.url <- sources$url[i]
i.node_type <- sources$node_type[i]
my_titles_raw <- news_retreive(url = i.url, node_type = i.node_type)
if((length(my_titles_raw) > 0)){
all_title <- unique(c(all_title,  text_prepro(x = my_titles_raw))) # remove duplicates
} else {
warning(paste("\n source", i, "couldn't be queried"))
}
setTxtProgressBar(pb, i)
all_title
}
stopCluster(workers)
x
all_title <- unlist(x)
all_title
length(all_title)
times <- 10
System.time(
pb <- txtProgressBar(min = 0, max = times, style = 3, width = 50, char = "=")
for(i in 1:times){
source('functions_source.R')
setTxtProgressBar(pb, i)
}
)
System.time(
pb <- txtProgressBar(min = 0, max = times, style = 3, width = 50, char = "=")
for(i in 1:times){
source('functions_source.R')
setTxtProgressBar(pb, i)
}
)
pb <- txtProgressBar(min = 0, max = times, style = 3, width = 50, char = "=")
source('functions_source.R')
source('financial_scrape.R')
times <- 5
System.time(
for(i in 1:times){
source('financial_scrape.R')
}
)
system.time()
system.time(
pb <- txtProgressBar(min = 0, max = times, style = 3, width = 50, char = "=")
for(i in 1:times){
source('financial_scrape.R')
setTxtProgressBar(pb, i)
}
)
times <- 5
system.time(
for(i in 1:times){
source('financial_scrape.R')
}
)
if(NROW(my_news) == 0) stop('no news to retreive')
rm(list = ls())
setwd("/home/ndrecord/Dropbox/r_FINNEWS")
# source('functions_dev.R')
source('functions_source.R')
suppressMessages(library(doParallel))
suppressMessages(library(foreach))
suppressMessages(library(stringr))
suppressMessages(library(rvest))
suppressMessages(library(tm))
ncore <- 8
warning('synonimus implementation')
workers <- makeCluster(ncore)
registerDoParallel(workers)
all_title <- c()
sources <- read.csv('sources.csv')
# N <- nrow(sources); sources <- sources[N,]; sources
if(NROW(sources) == 0) stop('no source to scrape')
cat('\n 1. retreiving news from', NROW(sources), 'sources \n ')
x <- foreach(i = 1:nrow(sources)) %dopar% {
i.url <- sources$url[i]
i.node_type <- sources$node_type[i]
my_titles_raw <- news_retreive(url = i.url, node_type = i.node_type)
if((length(my_titles_raw) > 0)){
all_title <- unique(c(all_title,  text_prepro(x = my_titles_raw))) # remove duplicates
} else {
warning(paste("\n source", i, "couldn't be queried"))
}
all_title
}
stopCluster(workers)
all_title <- unlist(x)
to_rem <- c('stock','market','read','min','opinion','say','high','low',
'share','price','bank','trade','record','new','report','invest',
'will','whi','buy','investor')
corpus <- iconv(all_title)
corpus <- Corpus(VectorSource(corpus))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, removeWords, to_rem)
tdm <- as.matrix(TermDocumentMatrix(corpus))
word_count <- apply(tdm, 1, sum)
today_trend <- data.frame(word_count[word_count > 10 & word_count < 20])
write.csv(today_trend, 'todays_trend.csv')
my_news <- list()
of_interest <- read.csv('companies.csv')[,1]
if (NROW(of_interest) == 0) stop('no topic to look for')
cat('\n\n 2. filtering out news \n')
pb <- txtProgressBar(min = 0, max = NROW(of_interest), style = 3, width = 50, char = "=")
for(i in 1:NROW(of_interest)){
i.titles <- all_title[grep(of_interest[i], all_title)]
if(length(i.titles) > 0) my_news[[of_interest[i]]] <- i.titles
setTxtProgressBar(pb, i)
}
if(NROW(my_news) == 0) stop('no news to retreive')
cat('\n\n 3. getting sentiment \n')
workers <- makeCluster(ncore)
registerDoParallel(workers)
x <- foreach(i = 1:length(my_news)) %dopar% {
my_news[[i]] <- get_sentiment(my_news[[i]])
my_news[[i]]
}
?foreach
x <- foreach(i = 1:length(my_news), .packages = tm) %dopar% {
my_news[[i]] <- get_sentiment(my_news[[i]])
my_news[[i]]
}
x <- foreach(i = 1:length(my_news), .packages = 'tm') %dopar% {
my_news[[i]] <- get_sentiment(my_news[[i]])
my_news[[i]]
}
x
unlist(x)
x
x
length(x)
dim(x)
names(x)
x[[1]]
my_news
length(my_news)
x[[1]]
x[[2]]
stopCluster(workers)
ncore <- 5
workers <- makeCluster(ncore)
registerDoParallel(workers)
x <- foreach(i = 1:length(my_news), .packages = 'tm') %dopar% {
my_news[[i]] <- get_sentiment(my_news[[i]])
my_news[[i]]
}
x
x[[1]]
length(x)
x <- foreach(i = 1:length(my_news), .packages = 'tm') %dopar% {
get_sentiment(my_news[[i]])
}
x
length(x)
stopCluster(workers)
my_news
x
unlist(x)
prova <- unlist(x)
prova
x
names(my_news)
x <- foreach(i = 1:length(my_news), .packages = 'tm') %dopar% {
get_sentiment(my_news[[i]])
}
workers <- makeCluster(ncore)
registerDoParallel(workers)
ncore
x <- foreach(i = 1:length(my_news), .packages = 'tm') %dopar% {
get_sentiment(my_news[[i]])
}
names(x) <- names(my_news)
x
source('~/Dropbox/r_FINNEWS/financial_par.R', echo=TRUE)
system.time(
for(i in 1:times){
source('financial_par.R')
}
)
times <- 5
times <- 5
system.time(
for(i in 1:times){
source('financial_par.R')
}
)
rm(list = ls())
setwd("/home/ndrecord/Dropbox/r_FINNEWS")
# source('functions_dev.R')
source('functions_source.R')
suppressMessages(library(doParallel))
suppressMessages(library(foreach))
suppressMessages(library(stringr))
suppressMessages(library(rvest))
suppressMessages(library(tm))
ncore <- 8
warning('unique issue?')
warning('synonimus implementation')
workers <- makeCluster(ncore)
registerDoParallel(workers)
all_title <- c()
sources <- read.csv('sources.csv')
# N <- nrow(sources); sources <- sources[N,]; sources
if(NROW(sources) == 0) stop('no source to scrape')
cat('\n 1. retreiving news from', NROW(sources), 'sources \n ')
x <- foreach(i = 1:nrow(sources)) %dopar% {
i.url <- sources$url[i]
i.node_type <- sources$node_type[i]
my_titles_raw <- news_retreive(url = i.url, node_type = i.node_type)
if((length(my_titles_raw) > 0)){
all_title <- unique(c(all_title,  text_prepro(x = my_titles_raw))) # remove duplicates
} else {
warning(paste("\n source", i, "couldn't be queried"))
}
all_title
}
stopCluster(workers)
all_title <- unlist(x)
to_rem <- c('stock','market','read','min','opinion','say','high','low',
'share','price','bank','trade','record','new','report','invest',
'will','whi','buy','investor')
corpus <- iconv(all_title)
corpus <- Corpus(VectorSource(corpus))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, removeWords, to_rem)
tdm <- as.matrix(TermDocumentMatrix(corpus))
word_count <- apply(tdm, 1, sum)
today_trend <- data.frame(word_count[word_count > 10 & word_count < 20])
write.csv(today_trend, 'todays_trend.csv')
my_news <- list()
of_interest <- read.csv('companies.csv')[,1]
if (NROW(of_interest) == 0) stop('no topic to look for')
of_interest
strsplit(of_interest, ', ')
of_interest <- read.csv('companies.csv')[,1]
of_interest <- strsplit(of_interest, ', ')
NROW(of_interest)
NROW(of_interest) == 0
cat('\n\n 2. filtering out news \n')
i <- 1
of_interest
i <- 26
of_interest[[i]]
grep(of_interest[[i]], all_title)
grep(c('ciao','miao'), 'ciao')
grep(c('ciao','miao'), 'miao')
grepl(c('ciao','miao'), 'miao')
?grepl
library(stringi)
?stri_detect_fixed
stri_detect_fixed('miao', c('ciao','miao'))
stri_detect_fixed(c('miao','any', c('ciao','miao'))
stri_detect_fixed(c('miao','any'), c('ciao','miao'))
grep('amazon', all_title)
grep(c('amazon'), all_title)
NROW(of_interest[[i]])
idx <- c()
i
j <- 1
idx
of_interest[[i]]
of_interest[[i]][j]
grep(of_interest[[i]][j], all_title)
j <- 2
grep(of_interest[[i]][j], all_title)
idx <- c()
for(j in 1:NROW(of_interest[[i]])){
idx <- c(idx, grep(of_interest[[i]][j], all_title))
}
all_title[idx,]
all_title[idx]
all_title <- unique(unlist(x))
my_news <- list()
of_interest <- read.csv('companies.csv')[,1]
of_interest <- strsplit(of_interest, ', ')
if (NROW(of_interest) == 0) stop('no topic to look for')
idx <- c()
for(j in 1:NROW(of_interest[[i]])){
idx <- c(idx, grep(of_interest[[i]][j], all_title))
}
idx
all_title[idx]
all_title
all_title[idx]
all_title[idx[1]] == all_title[idx[3]]
unique(all_title[idx])
x
unlist(x)
all_title <- unique(unlist(x))
NROW(all_title)
my_news <- list()
of_interest <- read.csv('companies.csv')[,1]
of_interest <- strsplit(of_interest, ', ')
if (NROW(of_interest) == 0) stop('no topic to look for')
cat('\n\n 2. filtering out news \n')
idx <- c()
for(j in 1:NROW(of_interest[[i]])){
idx <- c(idx, grep(of_interest[[i]][j], all_title))
}
idx
NROW(all_title)
NROW(unique(all_title))
duplicated(all_title)
which(duplicated(all_title))
all_title[idx]
duplicated(all_title[idx])
cat('\n\n 2. filtering out news \n')
pb <- txtProgressBar(min = 0, max = NROW(of_interest), style = 3, width = 50, char = "=")
for(i in 1:NROW(of_interest)){
idx <- c()
for(j in 1:NROW(of_interest[[i]])){
idx <- c(idx, grep(of_interest[[i]][j], all_title))
}
i.titles <- all_title[idx]
if(length(i.titles) > 0) my_news[[of_interest[i]]] <- i.titles
setTxtProgressBar(pb, i)
}
i
idx <- c()
i
of_interest[[i]]
NROW(of_interest[[i]])
j
of_interest[[i]][j]
idx <- c(idx, grep(of_interest[[i]][j], all_title))
i.titles
length(i.titles)
of_interest[i]
cat('\n\n 2. filtering out news \n')
pb <- txtProgressBar(min = 0, max = NROW(of_interest), style = 3, width = 50, char = "=")
for(i in 1:NROW(of_interest)){
idx <- c()
for(j in 1:NROW(of_interest[[i]])){
idx <- c(idx, grep(of_interest[[i]][j], all_title))
}
i.titles <- all_title[idx]
if(length(i.titles) > 0) my_news[[of_interest[[i]]]] <- i.titles
setTxtProgressBar(pb, i)
}
cat('\n\n 2. filtering out news \n')
pb <- txtProgressBar(min = 0, max = NROW(of_interest), style = 3, width = 50, char = "=")
for(i in 1:NROW(of_interest)){
idx <- c()
for(j in 1:NROW(of_interest[[i]])){
idx <- c(idx, grep(of_interest[[i]][j], all_title))
}
i.titles <- all_title[idx]
if(length(i.titles) > 0) my_news[[of_interest[[i]][1]]] <- i.titles
setTxtProgressBar(pb, i)
}
my_news
source('~/Dropbox/r_FINNEWS/financial_scrape.R', echo=TRUE)
source('~/Dropbox/r_FINNEWS/financial_scrape.R', echo=TRUE)
