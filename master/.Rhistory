all_title <- unique(c(all_title,  text_prepro(x = my_titles_raw))) # remove duplicates
} else {
warning(paste("\n source", i, "couldn't be queried"))
}
all_title
}
stopCluster(workers)
all_title <- unique(unlist(x))
to_rem <- c('stock','market','read','min','opinion','say','high','low',
'share','price','bank','trade','record','new','report','invest',
'will','whi','buy','investor','day','updat','delta','pay','top',
'expect','near','see','call','milion','can','may','financi',
'quarter','year','month', 'million','first','higher','more',
'start')
corpus <- iconv(all_title)
corpus <- Corpus(VectorSource(corpus))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, removeWords, to_rem)
tdm <- as.matrix(TermDocumentMatrix(corpus))
word_count <- apply(tdm, 1, sum)
word_count <- word_count[word_count > 5 & word_count < 20]
png('today_trend.png')
wordcloud(words = names(word_count),
freq = word_count,
max.words = 150,
random.order = F,
min.freq = 5,
colors = brewer.pal(8, 'Dark2'),
scale = c(5, 0.3),
rot.per = 0.7)
dev.off()
source('~/Dropbox/r_FINNEWS/master/financial_scrape.R', echo=TRUE)
setwd('~/Dropbox/r_FINNEWS/')
source('./master/functions_dev.R')
to_downd <- read.csv('./master/companies.csv')
to_downd <- to_downd[to_downd[,2] != '', ]
to_downd[,1] <- unlist(lapply(strsplit(to_downd[,1], ', '), '[[', 1))
Y <- data.frame()
all_files <- gsub('.csv', '', list.files('./score/'))
if(NROW(to_downd) == 0) stop('no url to look for')
for(i in 1:NROW(to_downd)){
# if the file exist: proceed, else: skip
if(to_downd[i,1] %in% all_files){
# download, write and read data in
i.url <- to_downd[i,2] # extract url to query
i.file <- paste0('./price/', to_downd[i,1], '.csv') # define file name
download.file(i.url, i.file) # download file
i.dat <- read.csv(i.file) # load prices
i.dat.sent <- read.csv(paste0('./score/', to_downd[i,1],'.csv'))[,-1] # load score
# process price data and merge to score
i.dat$gain <- gain.p(i.dat$Close) # compute gain
i.dat$Date <- as.character(as.Date(i.dat$Date) + 1) # add one day
i.dat$last_gain <- ifelse(c(i.dat$gain[-1], NA) >= 0, 1, 0) # add yesterday price
X <- merge(i.dat.sent, i.dat, by.x = 'time', by.y = 'Date')#, all.x = TRUE) # join data
X$stock <- rep(to_downd[i,1], nrow(X)) # add name of stock
Y <- rbind(Y, X)
}
}
Y[is.na(Y)] <- 0 # impute 0 values
zero_index <- which(Y$mean.sentiment == 0)# retrieve na index
n <- length(zero_index) # count
jittered_val <-rnorm(n, 0, 5e-3) # simulate rnorm values
Y$mean.sentiment[zero_index] <- jittered_val # assign
Y <- Y[ - which(abs(Y$gain) > 3 * sd(Y$gain)), ] # why should I remove outlier
cols <- rep('black', nrow(Y))
cols[zero_index] <- 'red'
plot(Y$mean.sentiment, Y$gain, col = cols)
fit <- lme4::glmer(
I(gain > 0.000) ~ mean.sentiment + last_gain + (1 | stock),
family = binomial,
dat = Y
)
summary(fit)
fit <- lme4::glmer(
I(gain > 0.000) ~ mean.sentiment * last_gain + (1 | stock),
family = binomial,
dat = Y
)
summary(fit)
# ---------- check OAT
Y
# ---------- check OAT
head(Y)
stocks <- unique(Y$stock)
s
stocks <- unique(Y$stock)
for(s in stocks){
iY <- Y[Y$stock == s, ]
}
iY
hist(Y$gain)
hist(log(Y$gain))
log(Y$gain)
log(Y$gain + 1)
ifit <- glm(log(gain + 1) ~ mean.sentiment * last_gain, data = it)
ifit <- glm(log(gain + 1) ~ mean.sentiment * last_gain, data = iY)
ifit
# ---------- check OAT
head(Y)
stocks <- unique(Y$stock)
for(s in stocks){
iY <- Y[Y$stock == s, ]
ifit <- glm(log(gain + 1) ~ mean.sentiment * last_gain, data = iY)
cat('\n\n', s)
summary(fit)
}
# ---------- check OAT
head(Y)
stocks <- unique(Y$stock)
for(s in stocks){
iY <- Y[Y$stock == s, ]
ifit <- glm(log(gain + 1) ~ mean.sentiment * last_gain, data = iY)
cat('\n\n', s)
print(summary(fit))
}
# ---------- check OAT
head(Y)
stocks <- unique(Y$stock)
for(s in stocks){
iY <- Y[Y$stock == s, ]
ifit <- glm(log(gain + 1) ~ mean.sentiment * last_gain, data = iY)
cat('--------------------------------------------- \n\n\n', s)
print(summary(fit))
}
library(Bergm)
bgof
fix(bgof)
rm(list = ls()) # clean env
path <- "/home/ndrecord/Dropbox/r_FINNEWS" # set path
setwd(path)
source('./master/packages_install.R') # run for packages installation (only first time)
path <- "/home/ndrecord/Dropbox/r_FINSCRAPE" # set path
setwd(path)
source('./master/packages_install.R') # run for packages installation (only first time)
source('./master/settings.R') # run to laod settings
source('./master/functions_source.R') # load useful functions
# load useful packages
suppressMessages(library(doParallel))
suppressMessages(library(wordcloud))
suppressMessages(library(foreach))
suppressMessages(library(stringr))
suppressMessages(library(rvest))
suppressMessages(library(tm))
options(warn=-1)
# set parallel workers
workers <- makeCluster(ncore)
registerDoParallel(workers)
all_title <- c() # initialize vector host
sources <- read.csv('./master/sources.csv') # load sources to be scraped
if(NROW(sources) == 0) stop('no source to scrape') # check presence of sources
# run parallel process
cat('\n 1. retreiving news from', NROW(sources), 'sources \n ')
x <- foreach(i = 1:nrow(sources)) %dopar% {
i.url <- sources$url[i] # extract url
i.node_type <- sources$node_type[i] # extract node type
# function to scrape news from the source
# making usage of the rvest library
my_titles_raw <- news_retreive(url = i.url, node_type = i.node_type)
if((length(my_titles_raw) > 0)){ # check for news presence
all_title <- unique(
c(
all_title,
# function to clean the text scraped from the web
# we apply regex, keep only those within a certain amount of characters,
# convert to lower case, remove duplicate and return the object
text_prepro(x = my_titles_raw) # apply text pre-processing
)
) # remove duplicates
} else {
warning(paste("\n source", i, "couldn't be queried"))
}
all_title # return object to main environment
}
stopCluster(workers) # stop parallel workers
all_title <- unique(unlist(x)) # store results in all_ttile
corpus <- iconv(all_title) # convert character vector
corpus <- Corpus(VectorSource(corpus)) # create corpus
corpus <- tm_map(corpus, stemDocument) # check for stem words
corpus <- tm_map(corpus, removeWords, stopwords('english')) # check for stop words
corpus <- tm_map(corpus, removeWords, to_rem) # check for words specified in settings.R
tdm <- as.matrix(TermDocumentMatrix(corpus)) # convert into incidence matrix
word_count <- apply(tdm, 1, sum) # count word frequency
word_count <- word_count[word_count > 5 & word_count < 20] # keep those between a certain range
# render word cloud
png('today_trend.png')
wordcloud(words = names(word_count),
freq = word_count,
max.words = 150,
random.order = F,
min.freq = 5,
colors = brewer.pal(8, 'Dark2'),
scale = c(5, 0.3),
rot.per = 0.7)
# stop rendering
dev.off()
my_news <- list() # initialize list
of_interest <- read.csv('./master/companies.csv')[,1] # load wording of interest
of_interest <- strsplit(of_interest, ', ') # create vectors of synonyms (if present)
if (NROW(of_interest) == 0) stop('no topic to look for') # check for presence
cat('\n\n 2. filtering out news \n')
pb <- txtProgressBar(min = 0, max = NROW(of_interest), style = 3, width = 50, char = "=")
# loop over the topics
for(i in 1:NROW(of_interest)){
idx <- c() # initialize vector
# loop over each topic words
for(j in 1:NROW(of_interest[[i]])){
# extract title index containing the word
idx <- c(idx, grep(of_interest[[i]][j], all_title))
}
# extract titles according to the index identified and remove duplicates
i.titles <- unique(all_title[idx])
# if any, attach to final list
if(length(i.titles) > 0) my_news[[of_interest[[i]][1]]] <- i.titles
setTxtProgressBar(pb, i) # update progress bar
}
my_news
days <- 7 # date range for older news
dates <- Sys.Date() - days:0
current_dir <- getwd() # store current dir
path <- './data/'
# extract names of the file to be written
filename <- sapply(names(my_news), function(x) paste0(x, '.csv'))
files <- list.files(path) # list available files
cat('\n\n 3. writing results \n ')
pb <- txtProgressBar(min = 0, max = NROW(filename), style = 3, width = 50, char = "=")
# for each topic
for(i in 1:length(my_news)){
# if file exits, load and append
if (filename[i] %in% files){
i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
i.dat.title <- i.dat[as.Date(i.dat$time) %in% dates, 'title'] # extract today's title
my_news[[i]] <- my_news[[i]][!(my_news[[i]] %in% i.dat.title)] # check existence of the news
if(NROW(my_news[[i]]) > 0){ # if there are new titles
i.dat <- rbind(my_news[[i]], i.dat) # add new titles
write.csv(i.dat, paste0(path, filename[i])) # write files
}
} else { # file doesn't exist, create
write.csv(my_news[[i]], paste0(path, filename[i])) # create new file
}
setTxtProgressBar(pb, i) # update progress bar
}
files <- list.files(path) # files to check
#  log activity and return final message ---------------------------------------
log <- read.csv('./master/log')[,-1]
log <- rbind(c(as.character(Sys.time()),'job executed'), log)
write.csv(log, './master/log')
cat('\n\n process executed correctly \n\n ')
source("~/Dropbox/r_FINSCRAPE/master/financial_scrape.R", echo=TRUE)
source("~/Dropbox/r_FINSCRAPE/master/financial_scrape.R", echo=TRUE)
days <- 7 # date range for older news
dates <- Sys.Date() - days:0
current_dir <- getwd() # store current dir
path <- './data/'
# extract names of the file to be written
filename <- sapply(names(my_news), function(x) paste0(x, '.csv'))
files <- list.files(path) # list available files
files
cat('\n\n 3. writing results \n ')
i <- 1
filename[i] %in% files
i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
i.dat
i.dat.title <- i.dat[as.Date(i.dat$time) %in% dates, 'title'] # extract today's title
i.dat
as.Date(i.dat$time)
i.dat
head(i.dat)
Sys.Date()
days <- 7 # date range for older news
dates <- Sys.Date() - days:0
current_dir <- getwd() # store current dir
path <- './data/'
# extract names of the file to be written
filename <- sapply(names(my_news), function(x) paste0(x, '.csv'))
files <- list.files(path) # list available files
cat('\n\n 3. writing results \n ')
pb <- txtProgressBar(min = 0, max = NROW(filename), style = 3, width = 50, char = "=")
# for each topic
for(i in 1:length(my_news)){
# if file exits, load and append
if (filename[i] %in% files){
i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
i.dat.title <- i.dat[as.Date(i.dat$time) %in% dates, 'title'] # extract today's title
my_news[[i]] <- my_news[[i]][!(my_news[[i]] %in% i.dat.title)] # check existence of the news
if(NROW(my_news[[i]]) > 0){ # if there are new titles
i.dat <- rbind(
c(Sys.Date(), my_news[[i]], i.dat) # add new titles
)
write.csv(i.dat, paste0(path, filename[i])) # write files
}
} else { # file doesn't exist, create
write.csv(my_news[[i]], paste0(path, filename[i])) # create new file
}
setTxtProgressBar(pb, i) # update progress bar
}
i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
i.dat
filename[i] %in% files
i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
i.dat
read.csv(paste0(path, filename[i]))
x <-  read.csv(paste0(path, filename[i]))
x
head(x)
list.files('./data/')
files <- list.files('./data/')
for(i in 1:length(files))
files <- list.files()
setwd('./datra')
setwd('./data')
files <- list.files()
x <- read.csv(files[1])
x
head(data)
head(x)
i <- 2
x <- read.csv(files[2])
head(x)
x <- x[-1, ]
x
head(x)
x <- read.csv(files[1])
x
x <- x[c('time','titles')]
x <- x[,c('time','titles')]
names(x)
x <- x[,c('time','title')]
x
for(i in 1:length(files)){
x <- read.csv(files[i])
x <- x[,c('time','title')]
write.csv(x, files[i])
}
i
x <- read.csv(files[i])
x
head(x)
getwd()
files <- list.files()
setwd('~/Dropbox/r_FINSCRAPE/data/')
files <- list.files()
files
x <- read.csv(files[i])
x
head(x)
i
source("~/.active-rstudio-document", echo=TRUE)
days <- 7 # date range for older news
dates <- Sys.Date() - days:0
current_dir <- getwd() # store current dir
path <- './data/'
# extract names of the file to be written
filename <- sapply(names(my_news), function(x) paste0(x, '.csv'))
getwd()
setwd('~/Dropbox/r_FINSCRAPE/')
days <- 7 # date range for older news
dates <- Sys.Date() - days:0
path <- './data/'
# extract names of the file to be written
filename <- sapply(names(my_news), function(x) paste0(x, '.csv'))
files <- list.files(path) # list available files
files
cat('\n\n 3. writing results \n ')
pb <- txtProgressBar(min = 0, max = NROW(filename), style = 3, width = 50, char = "=")
# for each topic
for(i in 1:length(my_news)){
# if file exits, load and append
if (filename[i] %in% files){
i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
i.dat.title <- i.dat[as.Date(i.dat$time) %in% dates, 'title'] # extract today's title
my_news[[i]] <- my_news[[i]][!(my_news[[i]] %in% i.dat.title)] # check existence of the news
if(NROW(my_news[[i]]) > 0){ # if there are new titles
i.dat <- rbind(
c(Sys.Date(), my_news[[i]], i.dat) # add new titles
)
write.csv(i.dat, paste0(path, filename[i])) # write files
}
} else { # file doesn't exist, create
write.csv(my_news[[i]], paste0(path, filename[i])) # create new file
}
setTxtProgressBar(pb, i) # update progress bar
}
i
i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
i.dat
head(i.dat)
i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
i.dat.title <- i.dat[as.Date(i.dat$time) %in% dates, 'title'] # extract today's title
my_news[[i]] <- my_news[[i]][!(my_news[[i]] %in% i.dat.title)] # check existence of the news
my_news[[i]]
c(Sys.Date(), my_news[[i]], i.dat) # add new titles
i.dat <- rbind(
c(rep(Sys.Date(), NROW(my_news[[i]]), my_news[[i]], i.dat) # add new titles
)
write.csv(i.dat, paste0(path, filename[i])) # write files
}
} else { # file doesn't exist, create
write.csv(my_news[[i]], paste0(path, filename[i])) # create new file
}
setTxtProgressBar(pb, i) # update progress bar
}
#  log activity and return final message ---------------------------------------
log <- read.csv('./master/log')[,-1]
log <- rbind(c(as.character(Sys.time()),'job executed'), log)
write.csv(log, './master/log')
cat('\n\n process executed correctly \n\n ')
c(rep(Sys.Date(), NROW(my_news[[i]]), my_news[[i]], i.dat) # add new titles
my_news
c(rep(Sys.Date(), NROW(my_news[[i]]), my_news[[i]], i.dat)
)
cbind(rep(Sys.Date(), NROW(my_news[[i]]), my_news[[i]], i.dat))
cbind(rep(Sys.Date(), NROW(my_news[[i]])), my_news[[i]], i.dat) # add new titles
rep(Sys.Date(), NROW(my_news[[i]]))
my_news[[i]]
cbind(rep(Sys.Date(), NROW(my_news[[i]])), my_news[[i]])
i.dat <- rbind(
cbind(rep(Sys.Date(), NROW(my_news[[i]])), my_news[[i]]),
i.dat # add new titles
)
i.dat <- rbind(
data.frame(
time = rep(Sys.Date(), NROW(my_news[[i]])),
title = my_news[[i]]
),
i.dat # add new titles
)
i.dat
my_news[[i]]
my_news <- list() # initialize list
of_interest <- read.csv('./master/companies.csv')[,1] # load wording of interest
of_interest <- strsplit(of_interest, ', ') # create vectors of synonyms (if present)
if (NROW(of_interest) == 0) stop('no topic to look for') # check for presence
cat('\n\n 2. filtering out news \n')
pb <- txtProgressBar(min = 0, max = NROW(of_interest), style = 3, width = 50, char = "=")
i <- 1
idx <- c() # initialize vector
j <-. 1
j <- 1
of_interest[[i]][j]
# extract title index containing the word
word_j <- paste0(' ', of_interest[[i]][j], ' ')
word_j
my_news <- list() # initialize list
of_interest <- read.csv('./master/companies.csv')[,1] # load wording of interest
of_interest <- strsplit(of_interest, ', ') # create vectors of synonyms (if present)
if (NROW(of_interest) == 0) stop('no topic to look for') # check for presence
cat('\n\n 2. filtering out news \n')
pb <- txtProgressBar(min = 0, max = NROW(of_interest), style = 3, width = 50, char = "=")
# loop over the topics
for(i in 1:NROW(of_interest)){
idx <- c() # initialize vector
# loop over each topic words
for(j in 1:NROW(of_interest[[i]])){
# extract title index containing the word
word_j <- paste0(' ', of_interest[[i]][j], ' ')
idx <- c(idx, grep(word_j, all_title))
}
# extract titles according to the index identified and remove duplicates
i.titles <- unique(all_title[idx])
# if any, attach to final list
if(length(i.titles) > 0) my_news[[of_interest[[i]][1]]] <- i.titles
setTxtProgressBar(pb, i) # update progress bar
}
my_news
my_news$apple
source("~/Dropbox/r_FINSCRAPE/master/financial_scrape.R", echo=TRUE)
i
days <- 7 # date range for older news
dates <- Sys.Date() - days:0
path <- './data/'
# extract names of the file to be written
filename <- sapply(names(my_news), function(x) paste0(x, '.csv'))
files <- list.files(path) # list available files
cat('\n\n 3. writing results \n ')
pb <- txtProgressBar(min = 0, max = NROW(filename), style = 3, width = 50, char = "=")
i <- 1
filename[i] %in% files
i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
i.dat
i.dat <- read.csv(paste0(path, filename[i]))
i.dat
filename[i]
i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
i.dat
x <- i.dat[, c('time','title')]
x
head(x)
write.csv(x, 'apple.csv')
i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
i.dat
i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
i.dat
head(i.dat)
i.dat.title <- i.dat[as.Date(i.dat$time) %in% dates, 'title'] # extract today's title
my_news[[i]] <- my_news[[i]][!(my_news[[i]] %in% i.dat.title)] # check existence of the news
if(NROW(my_news[[i]]) > 0){ # if there are new titles
i.dat <- rbind(
data.frame(
time = rep(Sys.Date(), NROW(my_news[[i]])),
title = my_news[[i]]
),
i.dat # add new titles
)
write.csv(i.dat, paste0(path, filename[i])) # write files
}
source("~/Dropbox/r_FINSCRAPE/master/financial_scrape.R", echo=TRUE)
?tm
??tm
library(tm)
??tm
tm()
