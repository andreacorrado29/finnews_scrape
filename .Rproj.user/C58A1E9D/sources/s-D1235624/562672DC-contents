
rm(list = ls())
setwd("/home/ndrecord/Dropbox/r_FINNEWS") 
source('functions_source.R')
suppressMessages(library(stringr))
suppressMessages(library(rvest))
suppressMessages(library(tm))

# next step ------------------------------------------------------------------

# warning('add social media as sources as well?')
# warning('add more source such as tech news or similar')
warning('in daily plot you need to match the dates, do sth like a join on time')

# download data from sources -------------------------------------------------

all_title <- c()
sources <- read.csv('sources.csv')
# N <- nrow(sources); sources <- sources[N,] 
if(NROW(sources) == 0) stop('no source to scrape')

cat('\n retreiving news \n ')
pb <- txtProgressBar(min = 0, max = nrow(sources), style = 3, width = 50, char = "=") 
for(i in 1:nrow(sources)){
  i.url <- sources$url[i]
  i.node_type <- sources$node_type[i]
  my_titles_raw <- news_retreive(url = i.url, node_type = i.node_type)
  all_title <- unique(c(all_title,  text_prepro(x = my_titles_raw))) # remove duplicates
  setTxtProgressBar(pb, i)
}

# filter data ----------------------------------------------------------------

my_news <- list()
of_interest <- read.csv('companies.csv')[,1]
if (NROW(of_interest) == 0) stop('no topic to look for')

cat('\n\n filtering out news \n')
pb <- txtProgressBar(min = 0, max = NROW(of_interest), style = 3, width = 50, char = "=") 
for(i in 1:NROW(of_interest)){
  i.titles <- all_title[grep(of_interest[i], all_title)]
  if(length(i.titles) > 0) my_news[[of_interest[i]]] <- i.titles
  setTxtProgressBar(pb, i)
}

# get sentiment --------------------------------------------------------------

if(NROW(my_news) == 0) stop('no news to retreive')
cat('\n\n getting sentiment \n')
pb <- txtProgressBar(min = 0, max = NROW(my_news), style = 3, width = 50, char = "=") 
for(i in 1:length(my_news)){
  my_news[[i]] <- get_sentiment(my_news[[i]])
  setTxtProgressBar(pb, i)
}

# warning('multiply mean sentiment by a values propto sqrt(1+counts),
#         so that we see how spread the news is')

# write result to file -------------------------------------------------------

# date range for older news
days <- 5
dates <- Sys.Date() - days:0

## move to a function

path <- './data/'
filename <- sapply(names(my_news), function(x) paste0(x, '.csv'))
files <- list.files(path)
cat('\n\n writing results \n ')
pb <- txtProgressBar(min = 0, max = NROW(filename), style = 3, width = 50, char = "=") 
for(i in 1:length(my_news)){
  if (filename[i] %in% files){ # file exits
    i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
    i.dat.title <- i.dat[as.Date(i.dat$time) %in% dates, 'title'] # extract today's title
    my_news[[i]] <- my_news[[i]][!(my_news[[i]]$title %in% i.dat.title),] # check existence of the news
    if(nrow(my_news[[i]]) > 0){ # if there are new titles
      i.dat <- rbind(my_news[[i]], i.dat) # add new titles
      write.csv(i.dat, paste0(path, filename[i])) # write files
    }
  } else { # file doesn't exist
    write.csv(my_news[[i]], paste0(path, filename[i]))
  }
  setTxtProgressBar(pb, i)
}

# estimate daily sentiment -----------------------------------------------------

files <- list.files(path) # files to check
comp <- gsub('.csv', '', files) # root 
if(NROW(comp) == 0) stop('\n\n no company found \n')
f <- function(x) mean(x) * sqrt(length(x)) # daily function
aggr <- mean # daily aggregation
ylim <- .4 * c(-1, 1)

dim_pic <- 1000
jpeg(file='daily_sentiment.jpeg', width = dim_pic*1.5, height = dim_pic)
cat('\n\n writing daily results \n ')
pb <- txtProgressBar(min = 0, max = NROW(comp), style = 3, width = 50, char = "=") 
for(i in 1:NROW(comp)){
  i.dat <- daily_aggr(comp = comp[i], f = f, aggr = aggr) # write information
  daily_plot(i.dat = i.dat, i = i, ylim = ylim)
  setTxtProgressBar(pb, i)
}
abline(h = 0, col = 'gray80', lty = 2)
legend('topleft', legend = comp, fill = 1:length(comp), bty = 'n')
dev.off()
  
  

# final message ----------------------------------------------------------------

cat('\n\n done \n\n')




