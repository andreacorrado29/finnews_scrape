
rm(list = ls())
setwd("/home/ndrecord/Dropbox/r_FINNEWS") 
# source('functions_dev.R')
source('functions_source.R')
suppressMessages(library(doParallel))
suppressMessages(library(foreach))
suppressMessages(library(stringr))
suppressMessages(library(rvest))
suppressMessages(library(tm))
options(warn=-1)
ncore <- 8


# next step ------------------------------------------------------------------

warning('unique issue?')
warning('synonimus implementation')
# warning('add social media as sources as well?')
# warning('add more source such as tech news or similar')

# download data from sources -------------------------------------------------

workers <- makeCluster(ncore)
registerDoParallel(workers)

all_title <- c()
sources <- read.csv('sources.csv')
# N <- nrow(sources); sources <- sources[N,]; sources
if(NROW(sources) == 0) stop('no source to scrape')

cat('\n 1. retreiving news from', NROW(sources), 'sources \n ')
x <- foreach(i = 1:nrow(sources)) %dopar% {
  i.url <- sources$url[i]
  i.node_type <- sources$node_type[i]
  my_titles_raw <- news_retreive(url = i.url, node_type = i.node_type)
  if((length(my_titles_raw) > 0)){
    all_title <- unique(c(all_title,  text_prepro(x = my_titles_raw))) # remove duplicates
  } else {
    warning(paste("\n source", i, "couldn't be queried"))
  }
  all_title
}
stopCluster(workers)
all_title <- unique(unlist(x))

# look for trend -------------------------------------------------------------

to_rem <- c('stock','market','read','min','opinion','say','high','low',
            'share','price','bank','trade','record','new','report','invest',
            'will','whi','buy','investor')

corpus <- iconv(all_title)
corpus <- Corpus(VectorSource(corpus))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeWords, stopwords('english'))
corpus <- tm_map(corpus, removeWords, to_rem)
tdm <- as.matrix(TermDocumentMatrix(corpus))
word_count <- apply(tdm, 1, sum)
today_trend <- data.frame(word_count[word_count > 10 & word_count < 20])
write.csv(today_trend, 'todays_trend.csv')

# filter data ----------------------------------------------------------------

my_news <- list()
of_interest <- read.csv('companies.csv')[,1]
of_interest <- strsplit(of_interest, ', ')
if (NROW(of_interest) == 0) stop('no topic to look for')


cat('\n\n 2. filtering out news \n')
pb <- txtProgressBar(min = 0, max = NROW(of_interest), style = 3, width = 50, char = "=") 
for(i in 1:NROW(of_interest)){
  idx <- c()
  for(j in 1:NROW(of_interest[[i]])){
    idx <- c(idx, grep(of_interest[[i]][j], all_title))
  }
  i.titles <- all_title[idx]
  if(length(i.titles) > 0) my_news[[of_interest[[i]][1]]] <- i.titles
  setTxtProgressBar(pb, i)
}

# get sentiment --------------------------------------------------------------

if(NROW(my_news) == 0) stop('no news to retreive')
cat('\n\n 3. getting sentiment \n')

workers <- makeCluster(ncore)
registerDoParallel(workers)

x <- foreach(i = 1:length(my_news), .packages = 'tm') %dopar% {
  get_sentiment(my_news[[i]])
}
names(x) <- names(my_news)
my_news <- x
stopCluster(workers)

# write result to file -------------------------------------------------------

days <- 7 # date range for older news
dates <- Sys.Date() - days:0

path <- './data/'
filename <- sapply(names(my_news), function(x) paste0(x, '.csv'))
files <- list.files(path)
cat('\n\n 4. writing results \n ')
pb <- txtProgressBar(min = 0, max = NROW(filename), style = 3, width = 50, char = "=") 
for(i in 1:length(my_news)){
  if (filename[i] %in% files){ # file exits
    i.dat <- read.csv(paste0(path, filename[i]))[,-1] # load data
    i.dat.title <- i.dat[as.Date(i.dat$time) %in% dates, 'title'] # extract today's title
    my_news[[i]] <- my_news[[i]][!(my_news[[i]]$title %in% i.dat.title),] # check existence of the news
    if(nrow(my_news[[i]]) > 0){ # if there are new titles
      i.dat <- rbind(my_news[[i]], i.dat) # add new titles
      write.csv(i.dat, paste0(path, filename[i])) # write files
    }
  } else { # file doesn't exist
    write.csv(my_news[[i]], paste0(path, filename[i]))
  }
  setTxtProgressBar(pb, i)
}

# estimate daily sentiment -----------------------------------------------------

files <- list.files(path) # files to check
comp <- gsub('.csv', '', files) # root 
if(NROW(comp) == 0) stop('\n\n 5. no company found \n')
f <- function(x) mean(x) * sqrt(length(x)) # daily function
aggr <- mean # daily aggregation
ylim <- .4 * c(-1, 1)

cat('\n\n 5. writing daily results \n ')
pb <- txtProgressBar(min = 0, max = NROW(comp), style = 3, width = 50, char = "=") 
for(i in 1:NROW(comp)){
  #i.dat <- daily_aggr(comp = comp[i], f = f, aggr = aggr) # write information
  i.dat <- period_aggr(comp = comp[i], f = f, aggr = aggr)
  setTxtProgressBar(pb, i)
}

# produce plot ------------------------------------------------------------------

th <- 6
rounds <- ceiling(length(comp) / th)
cat('\n\n 6. producing plots \n ')
pb <- txtProgressBar(min = 0, max = rounds, style = 3, width = 50, char = "=") 
for(j in 1:rounds){
  to_select <- ((j-1)*th + 1) : min(length(comp), (th *j)) # index to select
  j.comp <- comp[to_select] # extract companies
  d.filename <- paste0(j, '_sentiment_daily.png') # filename 
  w.filename <- paste0(j, '_sentiment_weekly.png') # filename
  period_plot(j.comp, metric = 'mean.sentiment', file = d.filename, ylim = .4 * c(-1, 1)) # run fun
  period_plot(j.comp, metric = 'mean.sentiment7', file = w.filename, ylim = 1 * c(-1, 1)) # run fun 
  setTxtProgressBar(pb, j)
}

#  log activity and return final message ---------------------------------------
log <- read.csv('log')[,-1]
log <- rbind(c(as.character(Sys.time()),'job executed'), log)
write.csv(log, 'log')

rm(list = ls()) # clean
cat('\n\n done \n\n') # done












